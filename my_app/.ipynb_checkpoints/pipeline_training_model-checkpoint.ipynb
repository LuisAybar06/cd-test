{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec434873-fb77-45bf-8dff-e182a1fc8df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Input, Output, component, Dataset)\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9232cd4f-e832-4ddb-8aaa-a6dcf5739824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "    \"google-cloud-bigquery\",\n",
    "    \"google-cloud-bigquery-storage\",\n",
    "    \"pandas\",\n",
    "    \"db-dtypes\",\n",
    "    \"pyarrow\"\n",
    "],\n",
    ")\n",
    "\n",
    "def process_data(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    data: Output[Dataset],\n",
    "):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    client = bigquery.Client(project = project)\n",
    "    \n",
    "    x_train = client.query(\n",
    "    '''SELECT * FROM `{dsorce_table}`'''.format(dsorce_table = source_x_train_table)).to_dataframe()\n",
    "    \n",
    "\n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsorce_table}`'''.format(dsorce_table = features_table))\n",
    "    \n",
    "    df = features.to_dataframe()\n",
    "    \n",
    "    features = df[\"string_field_0\"].tolist()\n",
    "    \n",
    "    x_train = x_train[features]\n",
    "    \n",
    "    x_train.to_parquet(f'{data.path}.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "10585a84-c16d-4f38-8750-2f00f401d6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "    \"google-cloud-bigquery\",\n",
    "    \"google-cloud-bigquery-storage\",\n",
    "    \"pandas\",\n",
    "    \"db-dtypes\",\n",
    "    \"scikit-learn\",\n",
    "    \"joblib\",\n",
    "    \"pandas-gbq\",\n",
    "    \"google-cloud-storage\",\n",
    "    \"pytz\"\n",
    "],\n",
    ")\n",
    "\n",
    "def prediction(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "):\n",
    "\n",
    "    import sys\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    import pandas_gbq\n",
    "    from joblib import load\n",
    "    from io import BytesIO\n",
    "    from pytz import timezone\n",
    "    \n",
    "    TZ = timezone(\"America/Lima\")\n",
    "    FORMAT_DATE = \"%Y-%m-%d\"\n",
    "    \n",
    "    client = bigquery.Client(project = project)\n",
    "    \n",
    "    x_train = client.query(\n",
    "    '''SELECT * FROM `{dsorce_table}`'''.format(dsorce_table = source_x_train_table)).to_dataframe()\n",
    "    \n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsorce_table}`'''.format(dsorce_table = features_table)).to_dataframe()\n",
    "    \n",
    "    \n",
    "    features = features[\"string_field_0\"].tolist()\n",
    "    \n",
    "    x_train = x_train[features]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_datetime_created():\n",
    "        return datetime.now()\n",
    "    \n",
    "    \n",
    "    def generate_date_created():\n",
    "        return datetime.now(TZ).date().strftime(FORMAT_DATE)\n",
    "    \n",
    "    \n",
    "    def load_model_from_gcs(path_model):\n",
    "        storage_client = storage.Client()\n",
    " \n",
    "        bucket_name, bloab_name = path_model.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "        \n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(bloab_name)\n",
    "        model_bytes = blob.download_as_string()\n",
    "        \n",
    "        classifier = load(BytesIO(model_bytes))\n",
    "        \n",
    "        return classifier\n",
    "    \n",
    "    \n",
    "    classifier = load_model_from_gcs(path_model)\n",
    "    \n",
    "    predictions = classifier.predict(x_train)\n",
    "    predictions = pd.DataFrame(predictions, columns = ['prediction'])\n",
    "    \n",
    "    user_id = client.query(\"SELECT SESSION_USER()\").to_dataframe().iloc[0,0]\n",
    "    \n",
    "    start_time = generate_datetime_created()\n",
    "    execute_date = generate_date_created()\n",
    "    \n",
    "    \n",
    "    predictions[\"creation_user\"] = user_id\n",
    "    predictions[\"process_date\"] = datetime.strptime(execute_date, '%Y-%m-%d')\n",
    "    predictions[\"process_date\"] = pd.to_datetime(predictions['process_date']).dt.date\n",
    "    predictions[\"load_date\"] = pd.to_datetime(start_time)\n",
    "    \n",
    "    pandas_gbq.to_gbq(predictions, table_id, if_exists = 'append', project_id = project)\n",
    "    \n",
    "    print(\"prediction done\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "616b242e-c7ce-49c5-a082-fad7ddcfc3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"pipeline-training-model\",\n",
    "    description=\"intro\",\n",
    "    pipeline_root=\"gs://vertex_mlops\"\n",
    ")\n",
    "\n",
    "\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "\n",
    "    prediction_task = prediction(\n",
    "        project = project,\n",
    "        source_x_train_table = source_x_train_table,\n",
    "        features_table = features_table,\n",
    "        table_id = table_id,\n",
    "        path_model = path_model,\n",
    "    )\n",
    "    prediction_task.set_display_name(\"PREDICTION_MODEL\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c6be6ae-96b8-489a-b159-cb72689bba28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler as v2_compiler\n",
    "\n",
    "\n",
    "v2_compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"pipeline_prediction_model.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751d2b0-284d-4d14-84a3-9d8bd2449624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7359d387-7ecd-4859-8a6b-d31b711a5043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a2f69c7-cfee-407c-aec8-7d2e76f55868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo pipeline_prediction_model.json subido a pipeline_prediction/pipeline_prediction_model.json en el bucket vertex_mlops.\n"
     ]
    }
   ],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"Archivo {source_file_name} subido a {destination_blob_name} en el bucket {bucket_name}.\")\n",
    "\n",
    "# Define las variables\n",
    "bucket_name = \"vertex_mlops\"\n",
    "destination_blob_name = \"pipeline_prediction/pipeline_prediction_model.json\"\n",
    "pipeline_file = \"pipeline_prediction_model.json\"\n",
    "# Llamar a la funci√≥n para subir el archivo\n",
    "upload_to_gcs(bucket_name, pipeline_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc4f0a-f57a-4f94-a05d-a023fbf354f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b7d60-4bd3-4940-958f-b923a0d37f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee7950be-2bc9-40a9-9ade-0da6597bda12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "25737c61-3378-4ce3-9e57-e86e79680259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"trim-odyssey-390415\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f238ecd5-9bd4-4bba-9fda-f9ba855221e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/668142834453/locations/us-central1/pipelineJobs/pipeline-training-model-20240817161222\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/668142834453/locations/us-central1/pipelineJobs/pipeline-training-model-20240817161222')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-training-model-20240817161222?project=668142834453\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "\n",
    "    display_name=\"pipeline de prediccion del modelo\",\n",
    "    template_path=\"gs://vertex_mlops/pipeline_prediction/pipeline_prediction_model.json\",\n",
    "    enable_caching=False,\n",
    "    project=\"trim-odyssey-390415\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"trim-odyssey-390415\",\n",
    "                      \"source_x_train_table\": \"trim-odyssey-390415.laybarm.xtrain\",\n",
    "                      \"features_table\": \"trim-odyssey-390415.laybarm.selected_features\",\n",
    "                      \"path_model\": \"gs://vertex_mlops/demo/data/model/model.joblib\",\n",
    "                      \"table_id\": \"trim-odyssey-390415.laybarm.predictions\"\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "\n",
    "job.submit(\"dev-mlops-vertex@trim-odyssey-390415.iam.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "dev",
   "name": "common-cpu.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m124"
  },
  "kernelspec": {
   "display_name": "dev (Local)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
